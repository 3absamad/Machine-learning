{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Heart Disease Prediction with Neural Networks**\n\nI chose this to be my very first Kernel on Kaggle, as in fact I quite enjoy the applications of Machine learning and how it impacts our lives. I am an *absolute beginner* as well and just breaking into the field. Thus, this kernel is more focused for learning purposes and exploration. Please feel free to give advice, recommendations/ better approaches or whatsover on the code below.\n\nThe heart disease data-set is a quite small dataset with today's modern standards. In this kernel first, I'll do a small bit of data exploration then, I try to create a very simple and straightforward model. That we analyze later, and try to improve its performance.\n\nFirst of all, lets add all packages that we will use."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import packages that we will be working with.\nimport os\nimport numpy as np\nimport pandas as pd\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(os.listdir(\"../input\"))\nnp.random.seed(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dataset**\n\nLets load the dataset, and check its components. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the dataset, and view couple of the first rows.\ndata = pd.read_csv(\"../input/heart.csv\")\nprint(data.head(3))\n\n# Check the datatypes\nprint(data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far we have a dataframe that containts all of our data. However we need to split to training data and target labels for learning. Then split the dataset into a train/test sets.\n\nWe do that in the following block of code."},{"metadata":{"trusted":true},"cell_type":"code","source":"# At this moment we have a dataframe that contains all of the heart.csv data. However we need to\n# Separate them to [X, Y]. Where our target labels are 'Y', and 'X' is our training data.\nY = data.target.values\nX = data.drop(['target'], axis=1)\n\n# Now split to train/test with 80% training data, and 20% test data.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Check dimensions of both sets.\nprint(\"Train Features Size:\", X_train.shape)\nprint(\"Test Features Size:\", X_test.shape)\nprint(\"Train Labels Size:\", Y_train.shape)\nprint(\"Test Labels Size:\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Neural Network Model**\n\nLets create a function that we can call later that builds our Neural Network model, and takes in the learning rate as a parameter. The architecture of the Neural Network that we're going to implement is detailed in the below illustration.\n![](https://raw.githubusercontent.com/3absamad/machine-learning/master/Kaggle/imgs/Kaggle_heart_disease_NN_model.png)\nIn our model, we use `Adam` (Adaptive Momentum) as our optimization algorithm, and set our metrics to `accuracy`. Furthermore, I have used the loss function to be `sparse_categorical_crossentropy` because our traget labels are *integers* and not one hot encoded.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a Neural Network Model\n\ndef NN_model(learning_rate):\n    model = Sequential()\n    model.add(Dense(32, input_dim=13, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    Adam(lr=learning_rate)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets build the NN-model and start training. I chose `learning_rate=0.01`, `epochs=100`, and `batch_size=16`. \n\nTraining the model for 100 epochs, seems to be pretty fine in order to avoid overfitting. I already performed training with 1000 epochs and around 100 epochs was the reasonable number of epochs for early stopping.\n\nLets take a look at our model summary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a NN-model, and start training\nlearning_rate = 0.01\nmodel = NN_model(learning_rate)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we train the network."},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=16, verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the `Model Accuracy`, and `Model Loss`  vs. the number of `Epochs`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the model accuracy vs. number of Epochs\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Train', 'Test'])\nplt.show()\n\n# Plot the Loss function vs. number of Epochs\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating our model's performance, we define the f1 score to be:\n\n$$f1 = \\frac{precision^{-1} + recall^{-1}}{2}$$\n\n\n$$precision = \\frac{\\textit{true positives}}{\\textit{true positives} + \\textit{false positives}}$$\n\n$$recall = \\frac{\\textit{true positives}}{\\textit{true positives} + \\textit{false negatives}}$$\n\nCheck this [Wikipedia](http://https://en.wikipedia.org/wiki/Precision_and_recall) page for more details on precision and recall.\n\nWe compute our model's predictions on the test set `X_test` and print a `classfication_report` from the imported package `sklearn.metrics`."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.argmax(model.predict(X_test), axis=1)\nmodel_accuracy = accuracy_score(Y_test, predictions)*100\nprint(\"Model Accracy:\", model_accuracy,\"%\")\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With a model accuracy of around 88% and very similar $f1$ score, we have a very good performing model that generalizes well to the test set."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}